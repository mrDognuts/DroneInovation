{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.1.0)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n",
      "Requirement already satisfied: detectron2 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: Pillow>=7.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (3.9.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (2.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (2.4.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (0.1.8)\n",
      "Requirement already satisfied: tabulate in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (3.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (4.66.4)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (2.16.2)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (0.1.9)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (1.3.2)\n",
      "Requirement already satisfied: black in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from detectron2) (24.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\walte\\appdata\\roaming\\python\\python312\\site-packages (from detectron2) (24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (6.0.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hydra-core>=1.1->detectron2) (4.9.3)\n",
      "Requirement already satisfied: portalocker in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from iopath<0.1.10,>=0.1.7->detectron2) (2.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->detectron2) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->detectron2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->detectron2) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->detectron2) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->detectron2) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\walte\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->detectron2) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\walte\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4.29.0->detectron2) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from black->detectron2) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from black->detectron2) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from black->detectron2) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\walte\\appdata\\roaming\\python\\python312\\site-packages (from black->detectron2) (4.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (70.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\walte\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard->detectron2) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->detectron2) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\walte\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->detectron2) (2.1.5)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\walte\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->iopath<0.1.10,>=0.1.7->detectron2) (306)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unregister dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# Unregister existing dataset if already registered\n",
    "if \"car_damage_train\" in DatasetCatalog.list():\n",
    "    DatasetCatalog.remove(\"car_damage_train\")\n",
    "    MetadataCatalog.remove(\"car_damage_train\")\n",
    "\n",
    "if \"car_damage_test\" in DatasetCatalog.list():\n",
    "    DatasetCatalog.remove(\"car_damage_test\")\n",
    "    MetadataCatalog.remove(\"car_damage_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegisterDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# Register the train and test datasets\n",
    "register_coco_instances(\"car_damage_train\", {}, \"Dataset/train/_annotations.coco.json\", \"Dataset/train\")\n",
    "register_coco_instances(\"car_damage_test\", {}, \"Dataset/test/_annotations.coco.json\", \"Dataset/test\")\n",
    "\n",
    "car_damage_metadata = MetadataCatalog.get(\"car_damage_train\")\n",
    "dataset_dicts = DatasetCatalog.get(\"car_damage_train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=car_damage_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    cv2.imshow(\"Sample Image\", vis.get_image()[:, :, ::-1])  # Corrected line\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()  # Close the window after displaying the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"car_damage_train\",)\n",
    "cfg.DATASETS.TEST = (\"car_damage_test\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 4  # Increase if your CPU has available cores\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1  # Reduce batch size for CPU\n",
    "cfg.SOLVER.BASE_LR = 0.0001  # Adjust learning rate for smaller batch size\n",
    "cfg.SOLVER.MAX_ITER = 1000  # Reduce iterations if needed\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64  # Reduce batch size per image\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
    "\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "cfg.OUTPUT_DIR = \"./output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using GPU: Ensure that you have the correct CUDA-enabled PyTorch installed.\n",
    "If using CPU: Modify the configuration to run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/27 21:31:03 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/27 21:31:03 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[08/27 21:31:03 d2.data.datasets.coco]: \u001b[0mLoaded 10218 images in COCO format from Dataset/train/_annotations.coco.json\n",
      "\u001b[32m[08/27 21:31:03 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 10218 images left.\n",
      "\u001b[32m[08/27 21:31:03 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|  damage-   | 0            | Car-Damage | 19181        |\n",
      "|            |              |            |              |\n",
      "|   total    | 19181        |            |              |\u001b[0m\n",
      "\u001b[32m[08/27 21:31:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[08/27 21:31:03 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[08/27 21:31:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[08/27 21:31:03 d2.data.common]: \u001b[0mSerializing 10218 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/27 21:31:03 d2.data.common]: \u001b[0mSerialized dataset takes 7.11 MiB\n",
      "\u001b[32m[08/27 21:31:03 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/27 21:31:03 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[08/27 21:31:03 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (12, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/27 21:31:04 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/27 21:33:04 d2.utils.events]: \u001b[0m eta: 1:33:21  iter: 19  total_loss: 2.303  loss_cls: 1.386  loss_box_reg: 0.3899  loss_rpn_cls: 0.1683  loss_rpn_loc: 0.02674    time: 5.8538  last_time: 5.7102  data_time: 0.1549  last_data_time: 0.0005   lr: 1.9981e-06  \n",
      "\u001b[32m[08/27 21:34:58 d2.utils.events]: \u001b[0m eta: 1:29:37  iter: 39  total_loss: 2.06  loss_cls: 1.364  loss_box_reg: 0.4817  loss_rpn_cls: 0.06189  loss_rpn_loc: 0.0127    time: 5.6955  last_time: 4.7619  data_time: 0.0006  last_data_time: 0.0006   lr: 3.9961e-06  \n",
      "\u001b[32m[08/27 21:36:53 d2.utils.events]: \u001b[0m eta: 1:29:25  iter: 59  total_loss: 2.159  loss_cls: 1.29  loss_box_reg: 0.6172  loss_rpn_cls: 0.03061  loss_rpn_loc: 0.01019    time: 5.7097  last_time: 5.2352  data_time: 0.0006  last_data_time: 0.0006   lr: 5.9941e-06  \n",
      "\u001b[32m[08/27 21:38:45 d2.utils.events]: \u001b[0m eta: 1:27:31  iter: 79  total_loss: 1.855  loss_cls: 1.162  loss_box_reg: 0.6739  loss_rpn_cls: 0.01408  loss_rpn_loc: 0.0109    time: 5.6818  last_time: 6.9907  data_time: 0.0006  last_data_time: 0.0005   lr: 7.9921e-06  \n",
      "\u001b[32m[08/27 21:40:35 d2.utils.events]: \u001b[0m eta: 1:25:54  iter: 99  total_loss: 1.327  loss_cls: 0.9858  loss_box_reg: 0.243  loss_rpn_cls: 0.07424  loss_rpn_loc: 0.01305    time: 5.6517  last_time: 4.3086  data_time: 0.0006  last_data_time: 0.0007   lr: 9.9901e-06  \n",
      "\u001b[32m[08/27 21:42:24 d2.utils.events]: \u001b[0m eta: 1:24:00  iter: 119  total_loss: 1.673  loss_cls: 0.9043  loss_box_reg: 0.6075  loss_rpn_cls: 0.09746  loss_rpn_loc: 0.02681    time: 5.6156  last_time: 4.2951  data_time: 0.0006  last_data_time: 0.0006   lr: 1.1988e-05  \n",
      "\u001b[32m[08/27 21:44:19 d2.utils.events]: \u001b[0m eta: 1:22:25  iter: 139  total_loss: 1.891  loss_cls: 0.8199  loss_box_reg: 0.6097  loss_rpn_cls: 0.1992  loss_rpn_loc: 0.01604    time: 5.6371  last_time: 6.9634  data_time: 0.0006  last_data_time: 0.0006   lr: 1.3986e-05  \n",
      "\u001b[32m[08/27 21:46:23 d2.utils.events]: \u001b[0m eta: 1:20:33  iter: 159  total_loss: 1.825  loss_cls: 0.7643  loss_box_reg: 0.9237  loss_rpn_cls: 0.02884  loss_rpn_loc: 0.01162    time: 5.6984  last_time: 7.2481  data_time: 0.0006  last_data_time: 0.0006   lr: 1.5984e-05  \n",
      "\u001b[32m[08/27 21:48:27 d2.utils.events]: \u001b[0m eta: 1:18:42  iter: 179  total_loss: 1.678  loss_cls: 0.7154  loss_box_reg: 0.881  loss_rpn_cls: 0.0233  loss_rpn_loc: 0.01177    time: 5.7593  last_time: 5.7224  data_time: 0.0006  last_data_time: 0.0005   lr: 1.7982e-05  \n",
      "\u001b[32m[08/27 21:50:28 d2.utils.events]: \u001b[0m eta: 1:16:50  iter: 199  total_loss: 1.172  loss_cls: 0.5488  loss_box_reg: 0.6219  loss_rpn_cls: 0.02914  loss_rpn_loc: 0.01011    time: 5.7886  last_time: 6.9760  data_time: 0.0006  last_data_time: 0.0005   lr: 1.998e-05  \n",
      "\u001b[32m[08/27 21:52:26 d2.utils.events]: \u001b[0m eta: 1:14:58  iter: 219  total_loss: 1.432  loss_cls: 0.5695  loss_box_reg: 0.6471  loss_rpn_cls: 0.0533  loss_rpn_loc: 0.01231    time: 5.7963  last_time: 4.9212  data_time: 0.0007  last_data_time: 0.0007   lr: 2.1978e-05  \n",
      "\u001b[32m[08/27 21:54:30 d2.utils.events]: \u001b[0m eta: 1:13:06  iter: 239  total_loss: 1.161  loss_cls: 0.4852  loss_box_reg: 0.5376  loss_rpn_cls: 0.04715  loss_rpn_loc: 0.01155    time: 5.8309  last_time: 4.9972  data_time: 0.0007  last_data_time: 0.0006   lr: 2.3976e-05  \n",
      "\u001b[32m[08/27 21:56:30 d2.utils.events]: \u001b[0m eta: 1:11:21  iter: 259  total_loss: 1.609  loss_cls: 0.5813  loss_box_reg: 0.9018  loss_rpn_cls: 0.02594  loss_rpn_loc: 0.01356    time: 5.8444  last_time: 5.5985  data_time: 0.0006  last_data_time: 0.0006   lr: 2.5974e-05  \n",
      "\u001b[32m[08/27 21:58:22 d2.utils.events]: \u001b[0m eta: 1:09:15  iter: 279  total_loss: 1.31  loss_cls: 0.4676  loss_box_reg: 0.6928  loss_rpn_cls: 0.04802  loss_rpn_loc: 0.01126    time: 5.8271  last_time: 7.4137  data_time: 0.0006  last_data_time: 0.0006   lr: 2.7972e-05  \n",
      "\u001b[32m[08/27 22:00:30 d2.utils.events]: \u001b[0m eta: 1:07:29  iter: 299  total_loss: 1.377  loss_cls: 0.4707  loss_box_reg: 0.4729  loss_rpn_cls: 0.0535  loss_rpn_loc: 0.01897    time: 5.8640  last_time: 5.1678  data_time: 0.0006  last_data_time: 0.0007   lr: 2.997e-05  \n",
      "\u001b[32m[08/27 22:02:34 d2.utils.events]: \u001b[0m eta: 1:05:44  iter: 319  total_loss: 1.276  loss_cls: 0.4608  loss_box_reg: 0.7022  loss_rpn_cls: 0.07122  loss_rpn_loc: 0.01175    time: 5.8844  last_time: 5.3757  data_time: 0.0007  last_data_time: 0.0005   lr: 3.1968e-05  \n",
      "\u001b[32m[08/27 22:04:37 d2.utils.events]: \u001b[0m eta: 1:03:59  iter: 339  total_loss: 1.258  loss_cls: 0.442  loss_box_reg: 0.7136  loss_rpn_cls: 0.03974  loss_rpn_loc: 0.0122    time: 5.9012  last_time: 6.4390  data_time: 0.0006  last_data_time: 0.0007   lr: 3.3966e-05  \n",
      "\u001b[32m[08/27 22:06:48 d2.utils.events]: \u001b[0m eta: 1:02:19  iter: 359  total_loss: 1.304  loss_cls: 0.4378  loss_box_reg: 0.7401  loss_rpn_cls: 0.05415  loss_rpn_loc: 0.008163    time: 5.9375  last_time: 7.3937  data_time: 0.0006  last_data_time: 0.0007   lr: 3.5964e-05  \n",
      "\u001b[32m[08/27 22:08:49 d2.utils.events]: \u001b[0m eta: 1:00:30  iter: 379  total_loss: 1.264  loss_cls: 0.4402  loss_box_reg: 0.697  loss_rpn_cls: 0.01727  loss_rpn_loc: 0.008278    time: 5.9431  last_time: 6.4183  data_time: 0.0006  last_data_time: 0.0005   lr: 3.7962e-05  \n",
      "\u001b[32m[08/27 22:10:52 d2.utils.events]: \u001b[0m eta: 0:58:41  iter: 399  total_loss: 0.9664  loss_cls: 0.3996  loss_box_reg: 0.503  loss_rpn_cls: 0.04534  loss_rpn_loc: 0.01378    time: 5.9544  last_time: 7.3053  data_time: 0.0006  last_data_time: 0.0007   lr: 3.996e-05  \n",
      "\u001b[32m[08/27 22:12:55 d2.utils.events]: \u001b[0m eta: 0:56:41  iter: 419  total_loss: 1.438  loss_cls: 0.4459  loss_box_reg: 0.8051  loss_rpn_cls: 0.03178  loss_rpn_loc: 0.01315    time: 5.9639  last_time: 7.6929  data_time: 0.0007  last_data_time: 0.0006   lr: 4.1958e-05  \n",
      "\u001b[32m[08/27 22:14:52 d2.utils.events]: \u001b[0m eta: 0:54:40  iter: 439  total_loss: 1.292  loss_cls: 0.4047  loss_box_reg: 0.7581  loss_rpn_cls: 0.02548  loss_rpn_loc: 0.01413    time: 5.9578  last_time: 5.5393  data_time: 0.0006  last_data_time: 0.0005   lr: 4.3956e-05  \n",
      "\u001b[32m[08/27 22:16:56 d2.utils.events]: \u001b[0m eta: 0:52:59  iter: 459  total_loss: 1.159  loss_cls: 0.3847  loss_box_reg: 0.6509  loss_rpn_cls: 0.05189  loss_rpn_loc: 0.009695    time: 5.9685  last_time: 4.5124  data_time: 0.0006  last_data_time: 0.0006   lr: 4.5954e-05  \n",
      "\u001b[32m[08/27 22:18:49 d2.utils.events]: \u001b[0m eta: 0:50:52  iter: 479  total_loss: 1.409  loss_cls: 0.3744  loss_box_reg: 0.963  loss_rpn_cls: 0.02153  loss_rpn_loc: 0.01328    time: 5.9562  last_time: 5.0944  data_time: 0.0006  last_data_time: 0.0005   lr: 4.7952e-05  \n",
      "\u001b[32m[08/27 22:20:52 d2.utils.events]: \u001b[0m eta: 0:49:03  iter: 499  total_loss: 1.265  loss_cls: 0.4063  loss_box_reg: 0.7835  loss_rpn_cls: 0.05036  loss_rpn_loc: 0.01181    time: 5.9628  last_time: 5.0091  data_time: 0.0007  last_data_time: 0.0008   lr: 4.995e-05  \n",
      "\u001b[32m[08/27 22:22:52 d2.utils.events]: \u001b[0m eta: 0:47:17  iter: 519  total_loss: 1.009  loss_cls: 0.3083  loss_box_reg: 0.6601  loss_rpn_cls: 0.02977  loss_rpn_loc: 0.01062    time: 5.9639  last_time: 4.5749  data_time: 0.0006  last_data_time: 0.0005   lr: 5.1948e-05  \n",
      "\u001b[32m[08/27 22:24:44 d2.utils.events]: \u001b[0m eta: 0:45:13  iter: 539  total_loss: 1.373  loss_cls: 0.4684  loss_box_reg: 0.6433  loss_rpn_cls: 0.09971  loss_rpn_loc: 0.0253    time: 5.9507  last_time: 5.5734  data_time: 0.0006  last_data_time: 0.0005   lr: 5.3946e-05  \n",
      "\u001b[32m[08/27 22:26:33 d2.utils.events]: \u001b[0m eta: 0:43:02  iter: 559  total_loss: 1.186  loss_cls: 0.358  loss_box_reg: 0.683  loss_rpn_cls: 0.05362  loss_rpn_loc: 0.01365    time: 5.9321  last_time: 4.4826  data_time: 0.0006  last_data_time: 0.0005   lr: 5.5944e-05  \n",
      "\u001b[32m[08/27 22:28:38 d2.utils.events]: \u001b[0m eta: 0:41:12  iter: 579  total_loss: 1.01  loss_cls: 0.3268  loss_box_reg: 0.583  loss_rpn_cls: 0.043  loss_rpn_loc: 0.008318    time: 5.9438  last_time: 7.2500  data_time: 0.0006  last_data_time: 0.0005   lr: 5.7942e-05  \n",
      "\u001b[32m[08/27 22:30:36 d2.utils.events]: \u001b[0m eta: 0:39:10  iter: 599  total_loss: 0.9338  loss_cls: 0.3378  loss_box_reg: 0.5561  loss_rpn_cls: 0.05602  loss_rpn_loc: 0.01468    time: 5.9431  last_time: 5.6625  data_time: 0.0006  last_data_time: 0.0005   lr: 5.994e-05  \n",
      "\u001b[32m[08/27 22:32:34 d2.utils.events]: \u001b[0m eta: 0:37:08  iter: 619  total_loss: 1.047  loss_cls: 0.35  loss_box_reg: 0.6856  loss_rpn_cls: 0.03207  loss_rpn_loc: 0.01053    time: 5.9413  last_time: 5.4836  data_time: 0.0006  last_data_time: 0.0009   lr: 6.1938e-05  \n",
      "\u001b[32m[08/27 22:34:39 d2.utils.events]: \u001b[0m eta: 0:35:19  iter: 639  total_loss: 1.307  loss_cls: 0.3379  loss_box_reg: 0.8607  loss_rpn_cls: 0.04595  loss_rpn_loc: 0.01458    time: 5.9504  last_time: 6.6202  data_time: 0.0006  last_data_time: 0.0006   lr: 6.3936e-05  \n",
      "\u001b[32m[08/27 22:36:36 d2.utils.events]: \u001b[0m eta: 0:33:15  iter: 659  total_loss: 1.121  loss_cls: 0.3308  loss_box_reg: 0.7181  loss_rpn_cls: 0.04203  loss_rpn_loc: 0.007794    time: 5.9480  last_time: 5.8388  data_time: 0.0006  last_data_time: 0.0006   lr: 6.5934e-05  \n",
      "\u001b[32m[08/27 22:38:37 d2.utils.events]: \u001b[0m eta: 0:31:23  iter: 679  total_loss: 0.999  loss_cls: 0.3417  loss_box_reg: 0.6723  loss_rpn_cls: 0.03761  loss_rpn_loc: 0.01407    time: 5.9504  last_time: 4.5807  data_time: 0.0006  last_data_time: 0.0007   lr: 6.7932e-05  \n",
      "\u001b[32m[08/27 22:40:30 d2.utils.events]: \u001b[0m eta: 0:29:23  iter: 699  total_loss: 0.8606  loss_cls: 0.262  loss_box_reg: 0.5445  loss_rpn_cls: 0.02765  loss_rpn_loc: 0.009728    time: 5.9419  last_time: 6.2584  data_time: 0.0006  last_data_time: 0.0006   lr: 6.993e-05  \n",
      "\u001b[32m[08/27 22:42:29 d2.utils.events]: \u001b[0m eta: 0:27:28  iter: 719  total_loss: 1.059  loss_cls: 0.3091  loss_box_reg: 0.7081  loss_rpn_cls: 0.03585  loss_rpn_loc: 0.01055    time: 5.9421  last_time: 7.1626  data_time: 0.0006  last_data_time: 0.0006   lr: 7.1928e-05  \n",
      "\u001b[32m[08/27 22:44:32 d2.utils.events]: \u001b[0m eta: 0:25:37  iter: 739  total_loss: 1.385  loss_cls: 0.35  loss_box_reg: 0.8017  loss_rpn_cls: 0.03257  loss_rpn_loc: 0.01157    time: 5.9486  last_time: 6.1276  data_time: 0.0006  last_data_time: 0.0006   lr: 7.3926e-05  \n",
      "\u001b[32m[08/27 22:46:32 d2.utils.events]: \u001b[0m eta: 0:23:42  iter: 759  total_loss: 1.039  loss_cls: 0.2897  loss_box_reg: 0.7254  loss_rpn_cls: 0.0232  loss_rpn_loc: 0.009352    time: 5.9499  last_time: 5.1943  data_time: 0.0006  last_data_time: 0.0005   lr: 7.5924e-05  \n",
      "\u001b[32m[08/27 22:48:38 d2.utils.events]: \u001b[0m eta: 0:21:52  iter: 779  total_loss: 1.321  loss_cls: 0.3498  loss_box_reg: 0.8196  loss_rpn_cls: 0.02735  loss_rpn_loc: 0.01317    time: 5.9581  last_time: 4.6828  data_time: 0.0006  last_data_time: 0.0006   lr: 7.7922e-05  \n",
      "\u001b[32m[08/27 22:50:34 d2.utils.events]: \u001b[0m eta: 0:19:52  iter: 799  total_loss: 1.16  loss_cls: 0.3531  loss_box_reg: 0.6163  loss_rpn_cls: 0.03461  loss_rpn_loc: 0.01211    time: 5.9545  last_time: 6.0624  data_time: 0.0006  last_data_time: 0.0005   lr: 7.992e-05  \n",
      "\u001b[32m[08/27 22:52:30 d2.utils.events]: \u001b[0m eta: 0:17:50  iter: 819  total_loss: 0.926  loss_cls: 0.2588  loss_box_reg: 0.6495  loss_rpn_cls: 0.03844  loss_rpn_loc: 0.007638    time: 5.9504  last_time: 5.0493  data_time: 0.0006  last_data_time: 0.0006   lr: 8.1918e-05  \n",
      "\u001b[32m[08/27 22:54:31 d2.utils.events]: \u001b[0m eta: 0:15:53  iter: 839  total_loss: 1.109  loss_cls: 0.3221  loss_box_reg: 0.7997  loss_rpn_cls: 0.04483  loss_rpn_loc: 0.01268    time: 5.9530  last_time: 7.1813  data_time: 0.0006  last_data_time: 0.0005   lr: 8.3916e-05  \n",
      "\u001b[32m[08/27 22:56:25 d2.utils.events]: \u001b[0m eta: 0:13:52  iter: 859  total_loss: 0.9457  loss_cls: 0.2718  loss_box_reg: 0.6058  loss_rpn_cls: 0.01784  loss_rpn_loc: 0.006467    time: 5.9472  last_time: 6.0663  data_time: 0.0008  last_data_time: 0.0054   lr: 8.5914e-05  \n",
      "\u001b[32m[08/27 22:58:32 d2.utils.events]: \u001b[0m eta: 0:11:58  iter: 879  total_loss: 1.076  loss_cls: 0.2629  loss_box_reg: 0.7681  loss_rpn_cls: 0.02994  loss_rpn_loc: 0.007016    time: 5.9562  last_time: 6.6822  data_time: 0.0006  last_data_time: 0.0007   lr: 8.7912e-05  \n",
      "\u001b[32m[08/27 23:00:28 d2.utils.events]: \u001b[0m eta: 0:09:56  iter: 899  total_loss: 1.028  loss_cls: 0.3019  loss_box_reg: 0.6713  loss_rpn_cls: 0.02118  loss_rpn_loc: 0.009483    time: 5.9532  last_time: 5.6029  data_time: 0.0007  last_data_time: 0.0006   lr: 8.991e-05  \n",
      "\u001b[32m[08/27 23:02:35 d2.utils.events]: \u001b[0m eta: 0:08:00  iter: 919  total_loss: 1.135  loss_cls: 0.3501  loss_box_reg: 0.6766  loss_rpn_cls: 0.03984  loss_rpn_loc: 0.0243    time: 5.9619  last_time: 7.4419  data_time: 0.0006  last_data_time: 0.0006   lr: 9.1908e-05  \n",
      "\u001b[32m[08/27 23:04:34 d2.utils.events]: \u001b[0m eta: 0:06:00  iter: 939  total_loss: 1.205  loss_cls: 0.2842  loss_box_reg: 0.8296  loss_rpn_cls: 0.04194  loss_rpn_loc: 0.009675    time: 5.9615  last_time: 4.5817  data_time: 0.0006  last_data_time: 0.0005   lr: 9.3906e-05  \n",
      "\u001b[32m[08/27 23:06:21 d2.utils.events]: \u001b[0m eta: 0:03:57  iter: 959  total_loss: 0.9487  loss_cls: 0.2623  loss_box_reg: 0.6851  loss_rpn_cls: 0.03557  loss_rpn_loc: 0.009972    time: 5.9488  last_time: 4.5382  data_time: 0.0006  last_data_time: 0.0006   lr: 9.5904e-05  \n",
      "\u001b[32m[08/27 23:08:23 d2.utils.events]: \u001b[0m eta: 0:01:59  iter: 979  total_loss: 1.217  loss_cls: 0.3579  loss_box_reg: 0.7628  loss_rpn_cls: 0.051  loss_rpn_loc: 0.01593    time: 5.9514  last_time: 6.1281  data_time: 0.0006  last_data_time: 0.0006   lr: 9.7902e-05  \n",
      "\u001b[32m[08/27 23:10:23 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 999  total_loss: 1.218  loss_cls: 0.3774  loss_box_reg: 0.7987  loss_rpn_cls: 0.04708  loss_rpn_loc: 0.01152    time: 5.9517  last_time: 5.1452  data_time: 0.0006  last_data_time: 0.0005   lr: 9.99e-05  \n",
      "\u001b[32m[08/27 23:10:23 d2.engine.hooks]: \u001b[0mOverall training speed: 998 iterations in 1:38:59 (5.9517 s / it)\n",
      "\u001b[32m[08/27 23:10:23 d2.engine.hooks]: \u001b[0mTotal training time: 1:39:04 (0:00:04 on hooks)\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/27 23:10:23 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[08/27 23:10:23 d2.data.datasets.coco]: \u001b[0mLoaded 486 images in COCO format from Dataset/test/_annotations.coco.json\n",
      "\u001b[32m[08/27 23:10:23 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|  damage-   | 0            | Car-Damage | 957          |\n",
      "|            |              |            |              |\n",
      "|   total    | 957          |            |              |\u001b[0m\n",
      "\u001b[32m[08/27 23:10:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[08/27 23:10:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[08/27 23:10:23 d2.data.common]: \u001b[0mSerializing 486 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/27 23:10:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.35 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/27 23:10:23 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fvcore\\common\\checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# Load the configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Ensure this matches the number of classes in your dataset\n",
    "\n",
    "# Load the trained model weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.DEVICE = \"cpu\"  # Use CPU or \"cuda\" for GPU\n",
    "\n",
    "# Set the model to inference mode\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a threshold for predictions\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Now you can use the predictor for inference\n",
    "# Example:\n",
    "# outputs = predictor(im)  # where 'im' is your input image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m build_detection_test_loader(cfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar_damage_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Perform the evaluation\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43minference_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)  \u001b[38;5;66;03m# Print the evaluation metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\evaluation\\evaluator.py:165\u001b[0m, in \u001b[0;36minference_on_dataset\u001b[1;34m(model, data_loader, evaluator, callbacks)\u001b[0m\n\u001b[0;32m    163\u001b[0m start_compute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mget(callbacks \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[1;32m--> 165\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mget(callbacks \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\meta_arch\\rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, batched_inputs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\meta_arch\\rcnn.py:208\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[1;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m         proposals, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\proposal_generator\\rpn.py:454\u001b[0m, in \u001b[0;36mRPN.forward\u001b[1;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[0;32m    451\u001b[0m features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]\n\u001b[0;32m    452\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator(features)\n\u001b[1;32m--> 454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n\u001b[0;32m    456\u001b[0m pred_objectness_logits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     score\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m pred_objectness_logits\n\u001b[0;32m    460\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\proposal_generator\\rpn.py:174\u001b[0m, in \u001b[0;36mStandardRPNHead.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    172\u001b[0m pred_anchor_deltas \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m--> 174\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     pred_objectness_logits\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjectness_logits(t))\n\u001b[0;32m    176\u001b[0m     pred_anchor_deltas\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_deltas(t))\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\layers\\wrappers.py:142\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    137\u001b[0m                 \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/12013\u001b[39;00m\n\u001b[0;32m    138\u001b[0m                 \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    139\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSyncBatchNorm\n\u001b[0;32m    140\u001b[0m                 ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyncBatchNorm does not support empty inputs!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 142\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = COCOEvaluator(\"car_damage_test\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"car_damage_test\")\n",
    "\n",
    "# Perform the evaluation\n",
    "metrics = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "print(metrics)  # Print the evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fvcore\\common\\checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import cv2\n",
    "\n",
    "# Load the trained model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Path to the trained model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a threshold for predictions\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Load an image for inference\n",
    "image_path = \"Dataset/test/severe_112_JPEG_jpg.rf.c2ff96a66830da8c1c96d9a11dba6302.jpg\"\n",
    "im = cv2.imread(image_path)\n",
    "\n",
    "# Run the predictor\n",
    "outputs = predictor(im)\n",
    "\n",
    "# Visualize the predictions on the image\n",
    "v = Visualizer(im[:, :, ::-1], metadata=car_damage_metadata, scale=0.8)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2.imshow(\"Prediction\", v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
